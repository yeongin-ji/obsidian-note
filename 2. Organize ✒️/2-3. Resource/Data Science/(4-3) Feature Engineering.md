>(1) 날짜: 2025.04.03
>(2) 태그(주제/카테고리): #datascience #preprocessing #process 

>(3) Link
- [[(4) Data Preprocessing]]
---

> Feature를 생성하고, 선택하고, 줄이는 과정을 통해서 최종적으로 해당 프로젝트에 사용할 데이터셋의 **feature를 결정하는 작업**을 의미한다. 이는 매우 중요함과 동시에 도메인 **전문 지식**이 필요하기 때문에 어려운 과정이기도 하다. **Python Featuretools**와 같은 라이브러리를 활용하여 이러한 작업을 더 쉽게 수행할 수도 있다.

*\*용어 정의:*
*Transformation - 하나의 Entity 내에 이미 존재하는 feature를 통해 새로운 feature를 만드는 것*
*Aggregation - 서로 다른 Entity를 병합하거나 통계적인 계산을 통해 하나의 엔티티 행 그룹을 집계하여 다른 엔티티의 feature로 병합하는 것*
# Feature creation
> 프로젝트 목표 달성과 관련된 모든 feature를 **획득**하고 **생성**하는 과정을 의미한다.
## 이미 존재하는 feature에서 새로운 feature 만들기
### 하나의 Entity 내에서 만들기
- **수치형 데이터(numerical data)에서 새로운 범주형 데이터(categorical data) 생성**: 연속형 데이터인 행복 지수를 범주화하여 행복 정도를 나타내는 새로운 feature를 생성할 수 있다.
- **범주형 데이터에서 새로운 범주형 데이터 생성:** 범주형 데이터인 나라 이름(country)에서 상위 개념으로 대륙 단위 범주형 데이터인 region feature를 새로 생성할 수 있다.
- **구조적 데이터(Structured data)를 여러 개의 feature로 분리:** 타임스탬프(1/14/17 23:11)를 6개(year/ month/ day/ date/ hour/ minute)의 feature로 **분리**할 수 있다.
- **특정 feature를 목적에 맞게 가공하여 추가 생성:** joined(2002-04-16/ 2007-11-14/ ...) 및 income(172849/ 184208/ 43851/ ...) feature에서 **월(month)에 대한 정보만 추출**한 join_month, 큰 값에 **log를 취한** log_income을 목적에 따라 추가 생성할 수 있다.
### 관련된 서로 다른 Entity에서 만들기
- **Aggregation을 통해 entity 병합하면서 새로운 feature 생성:** 두 개의 entity(client, loan)를 예로 들 수 있다. 각 client는 여러 개의 loan을 가질 수 있으므로, loan entity에서 같은 client간 금액 데이터를 통계적인 계산으로 **집계**할 수 있을 것이다. (mean/ min/ max/ ...) 이 데이터를 client entity에 새로운 feature로 추가함으로써 두 entity를 **병합**할 수 있다.
## 값을 변환하여 새로운 feature 만들기
- 머신러닝 알고리즘에는 수치 데이터만 입력할 수 있기 때문에 범주형 데이터에서 수치형 데이터로의 변환이 필요하다.
- N개의 값을 가지는 범주형 feature를 N개의 **수치(numerical)** 값을 가지는 새로운 feature로 만들 수 있다. 대표적으로 텍스트 및 이미지 데이터를 수치화하는 데 사용한다.
- **텍스트**의 경우, N개의 단어를 가지는 **말뭉치(Corpus)** 를 N개의 원소를 가진 벡터로 변환할 수 있다.
- **이미지**의 경우, 각 **픽셀 값**을 새로운 feature로 만들 수 있다.
### 원-핫 인코딩(One-Hot Encoding)
- N개의 가능한 값을 가지는 하나의 범주형 feature를 **N개의 새로운 수치형 feature**로 나타내는 인코딩 기법이다. 각 feature는 **0 혹은 1** 값을 가질 수 있다.
- 예를 들어, **sunny/ cloudy/ snow/ rain** 값 중 하나를 가질 수 있는 하나의 **Weather** feature가 있다고 해 보자. 이를 **원-핫 인코딩** 기법으로 나타내면 **w-sunny/ w-cloudy/ w-snow/ w-rain** 라는 이름으로 4개의 새로운 feature를 생성하며, 각 feature는 **해당하는 날씨에 대해서만 1로 표현**하고 **나머지는 모두 0**으로 표현한다.
- 이러한 기법에는 두 가지 **단점**이 있다.
	 1. feature수가 늘어남에 따라 **처리 시간**도 증가한다.
	 2. **Sparse matrix** 문제: 차원 수가 늘어남에 따라 0으로 표현해야 하는 데이터가 늘어나면서 **1 값이 희소해진다.** 이에 따라 0으로 채워진 불필요한 공간이 늘어나게 되어 저장 공간을 낭비하게 된다. 이러한 문제와 더불어 머신러닝에서 발생할 수 있는 또 다른 문제가 있는데, 이 내용은 아래 **"차원의 저주(Curse of dimension)"** 에서 연결하여 설명한다.
- **차원의 저주(Curse of dimension)**
	- 다음의 제품 데이터셋 **feature 항목**에서 어떤 feature가 **가격(price)** 에 가장 영향을 많이 미치는지 본다고 해 보자.
	- feature 항목: **ram size/ camera pixel/ n_cores(independent variable) | price(target variable)** *\*target var: 종속변수(예측하려는 변수), independent var: 독립변수(예측에 사용되는 정보)*
	- 지금은 독립 변수가 3개이므로 상관 관계를 어려움 없이 관찰할 수 있지만, **100개 혹은 그 이상**으로 늘어난다면 쉽게 상관 관계를 파악할 수 있을까?
	- 단순히 육안으로 관찰이 어렵다는 문제를 넘어서, 차원 수가 너무 많은 데이터로 머신러닝 모델을 학습하는 경우 다음과 같은 **문제**가 발생할 수 있다.
		1. 모델이 더 **복잡**해짐에 따라 학습 데이터에 **과적합(overfitting)** 될 수 있다. 이는 정확도를 하락시키는 원인이 된다.
		2. 데이터가 **희소**해짐에 따라 통계적으로 유의미한 정보를 얻지 못할 수 있다.
		3. **차원 수**가 많아짐에 따라 해당 공간에서의 **밀집도(density)** 나 **거리(distance)** 정보가 무의미해질 수 있다. *\*차원 수가 너무 높고 데이터가 희소한 공간에서는 어떤 벡터와의 거리를 계산해도 비슷하기 때문에 벡터 간 거리 차이가 **0으로 수렴**한다.*

# Feature selection
> 주어진 데이터셋에서 **비즈니스 목적**에 가장 부합하는 feature를 **선택**하는 프로세스이다. 좀 더 정확히는, 데이터셋이 가진 feature 집합의 원소 N개 중에서 k개의 원소를 가지는 **최소 부분집합**을 **선택**하는 과정이다. 
## Filter Methods
- 머신러닝 알고리즘을 실행하기 **이전**에 **통계적인 방법(상관 계수, 분산 등)** 을 이용하여 가장 좋은 feature subset을 선택하는 방법이다.
- 이 방식은 Target variable과 각각의 Independent variable의 **일대일 상관 관계만을 고려**하기 때문에 여러 feature의 조합을 고려하지 못한다는 단점이 있다.
- 예를 들어, 시험 점수를 예측할 때 공부 시간이 가장 상관 관계가 높다고 하더라도 수면 시간이 너무 낮은 경우 높은 공부 시간에도 불구하고 예측 결과 정확도가 떨어질 수 있다.
## Wrapper Methods
- Feature selection을 자동화하는 **또 다른 데이터 마이닝 알고리즘(혹은 머신러닝 모델)** 을 사용한다.
- 이 모델에 여러 개의 feature 조합을 입력하고, 그에 따른 모델 성능을 비교하여 가장 좋은 feature subset을 채택한다.
- Feature selection 모델에서 좋은 성능을 보였다고 하더라도 최종 목적 모델에서의 성능이 좋지 않을 수 있기 때문에, **최종 평가 지표**도 반드시 고려해야 한다.
## Embedded Methods
- Feature selection이 모델 학습 이전이 아니라 학습 **진행 중**에 수행되도록 하는 방법이다.
- 모델이 학습하는 과정에서 불필요한 feature들을 자동으로 제외하므로 Filter 및 Wrapper 방식보다 더 빠르고 효율적일 수 있다.
- 예를 들어, **L1 정규화 선형 회귀(L1-regularized linear regression)** 는 중요하지 않은 feature의 **가중치(weight)** 를 0으로 만들어서 학습 과정 중 자연스럽게 제거한다.
## 각 방식 별 시점 기준으로의 분류
| 시점(Timing) | 방식(Method)             |
| ---------- | ---------------------- |
| **학습 전**   | Filter, Wrapper method |
| **학습 중**   | Embedded method        |

# Feature reduction
> 데이터를 변환하여 많은 정보가 **소수의 feature에 집중**되도록 함으로써 feature 수를 **줄이는** 과정을 의미한다. feature를 줄인다는 측면에서는 Feature selection과 비슷하지만 feature 자체를 **변형**시킨다는 점에서 차이점이 있다.

## Principal Component Analysis (PCA)
- 데이터를 변형함으로써 많은 정보가 소수의 feature에 집중되도록 하는 data reduction 기법으로, **차원 축소**를 통해 머신러닝 **처리 속도를 증가**시키고 데이터를 **시각화**하는 것을 돕는다.
- 더 자세히 말하면, **많은** p개의 feature를 가진 데이터를 **더 작은** k개의 파생 feature로 **요약**하는 과정으로 설명할 수 있다. 이때 파생되는 feature는 feature selection과 달리 변형된 **복합적인(synthetic, composite)** feature이다.
- Karl Pearson(1901)과 Harold Hotelling(1933)에 의해 개발되었으며, **Multivariate data**를 분석하는 방법 중 가장 잘 알려지고 많이 사용된다.
	*\*Univariate/ Bivariate/ Multivariate data: feature 개수가 (1개/ 2개/ 3개 이상)인 데이터*
### Principal Component(PC)는 무엇일까?
> Principal component = 선형대수학의 **고유벡터(Eigenvector)**

- **고유벡터(Eigenvector)** 는 선형대수학에서 특정 행렬에 의해 **변형**되었을 때 **방향을 유지**하는 벡터를 의미한다. PCA에서 이 행렬은 **공분산 행렬(Covariance matrix)** 이다.
- **공분산(Covariance)** 은 서로 다른 두 변수의 관계성을 나타내는 수치이다. 즉, 두 변수가 **얼마나 함께 움직이는지**를 측정한다.
- **PCA의 목적**은 이러한 공분산 행렬의 고유벡터 중 가장 큰 고유값에 대응하는 **주성분 벡터(Principal Component)** 방향으로 기존 데이터를 투영함으로써, 차원을 축소하면서도 분산을 최대한 보존하는 것이다.
### 분산을 왜 보존하려고 할까?
- **분산(Variance)** 은 각 데이터가 평균으로부터 얼마나 떨어져 있는지에 대한 값이다. 분산이 클수록 데이터가 **많이** 흩어져 있다는 의미이다.
- 데이터가 많이 흩어져 있다는 것은 비슷한 값으로 모여 있는 데이터에 비해 **일반적으로 더 많은 정보량**을 가진다고 생각하기 때문에, 차원을 축소하는 과정에서 분산이 **더 큰** 방향을 보존하는 것이 유리하다.
- 예를 들어, 몸무게 데이터셋에서 각 몸무게 값이 다양하게 분포되어 있다면 해당 데이터의 **패턴**이 더 잘 드러날 것이고, 이는 **더 많은 정보**를 포함하고 있음을 시사한다. **반면,** 값의 차이가 없이 모두 동일하다면 **패턴**을 분석할 수 없고, 따라서 **무의미**한 데이터가 된다.
### 예외 상황은 없을까?
- **이상 탐지(Anomaly detection)** 상황에서는 PCA 차원 축소 기법이 부정적인 영향을 미칠 수 있다.
- 예를 들어, 공장에서 온도 센서가 항상 같은 온도를 유지해야 할 때, 시간에 따른 온도 값은 **거의 같을** 것이다. 이 시스템의 목적은 온도가 급격하게 변화하는 상황을 탐지하는 것이므로 **이상치(Outlier)** 가 중요한 정보이지만, PCA를 통해 차원을 축소하는 경우 중요한 데이터가 소실될 수 있다.
- 따라서 **비즈니스 목적**에 맞는 Feature Reduction 방법을 선택하는 것이 유리하다.
### PCA Process (PC를 구하는 방법)
1. 데이터 **정규화(Normalization)**
	- **공분산(Covariance)** 은 단위에 민감하기 때문에, 정규화를 통해 변수 간 단위 차이의 영향을 제거하고, 공정한 비교가 가능한 형태로 데이터를 변환해야 한다.
2. 정규화된 데이터의 **공분산 행렬(Covariance matrix)** 계산
	- 공분산 행렬은 항상 **대칭행렬(Symmetric matrix)** 이다.
3. 공분산 행렬의 **고유벡터(Eigenvector)** 와 대응되는 **고유값(Eigenvalue)** 계산
	- 대칭행렬의 고유값은 항상 **실수**이다.
	- 서로 다른 고유값에 대응되는 고유벡터는 서로 **직교(Orthogonal)** 하며, 이를 통해 중복 없는 독립된 방향(축)을 정의할 수 있다.
4. 고유값 기준 **상위 k개**의 고유벡터 선택 **(=주성분, Principal Component)**
	- PCA에서의 고유값은 고유 벡터를 따라 데이터에 얼마나 **많은** 분산이 있는지 나타낸다.
	- **주성분 개수**에 따른 분산 보존 정도 그래프를 보면, 몇 개의 주성분을 선택하는 것이 효율적인지 판단할 수 있다. 
	- 주성분 개수를 **늘릴수록** 누적 분산 비율은 증가하지만, 어느 시점 이후로는 증가 속도가 **급격히 줄어들게** 된다. 
	- 이러한 꺾이는 지점을 **엘보 포인트(Elbow Point)** 라고 하며, 이는 적절한 주성분 개수를 선택하는 데 유용한 기준이 된다.
5. 기존 데이터를 **새로운 축(주성분 축)** 으로 투영
	- 최종적으로, 기존 데이터를 선택한 주성분 축으로 **투영**하여 차원은 축소하되 정보(분산)는 최대한 **보존**한 새로운 표현 공간으로 **변환**한다.

## 그 외 다른 방법들
- Singular Value Decomposition
- Linear Discriminant Analysis
- Autoencoder (neural network)
- Self-Organizing Map (SOM)


>(5) Reference

