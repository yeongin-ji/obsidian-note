>(1) 날짜: 2025.04.03
>(2) 태그(주제/카테고리): #datascience #preprocessing #process 

>(3) Link
- [[(4) Data Preprocessing]]
---

> 막 수집된 데이터에는 여러 유형의 **Dirty data**가 있으므로 이를 제거하는 과정이 필요하다. 이 단계에서는 값을 변경하는 작업을 통해 이를 수행한다.

# Cleaning dirty data
dirty data는 **30가지**가 넘는 유형이 있지만, 주요한 타입만 짚고 넘어가도록 하자.
## Missing data
- **값 자체가 빠진 경우**를 의미한다. 데이터셋에 missing data가 너무 많을 경우 **전체**를 사용하지 못하게 될 수 있다. 일반적으로 `null` 값으로 처리한다.
- **값이 빠지는 이유**는 다음과 같은 두 가지로 나눌 수 있다.
	- 정말로 **값이 없어서** 기입하지 못한 경우
	- 유저가 **실수**로, 혹은 **의도적**으로 값을 제공하지 않은 경우
- **빠진 값을 처리하는 방법**은 다음과 같은 두 가지로 나눌 수 있다.
	- 빠진 부분의 record나 column을 단순히 데이터셋에서 **제외(drop/ignore)** 한다.
	- 빠진 부분을 다른 값으로 **대체**한다.
- **Missing data**를 무시하면 어떤 일이 일어날까?
	- Ignore the **feature** : 단순하고 데이터의 **편향**을 우려하지 않아도 되지만, 분석에 있어 **중요한 Feature를 누락**할 수 있다.
	- Ignore the **sample** : 이 역시 단순하지만, sample이 **한 쪽으로 치우치는 경향**이 있고, 분석하고자 하는 **전체 데이터셋 크기**가 작아질 수 있다.
- Missing data를 대체하는 방법 : Record 전체에 적용하거나, **클래스** 별로 적용하기도 한다.
	- **평균값(Mean)** 으로 대체
	- **중앙값(Median)** 으로 대체
	- **빈도수(Mode)** 로 대체
- **클래스**를 나누는 방법
	- **KNN(K-Nearest Neighbors)** : 데이터를 클러스터링하여 각 클러스터 별 **most probable value(median, mean, mode)** 를 계산한다.
- **Pandas**를 이용한 Missing data handling
	- `isna()`/ `dropna()`/ `fillna()`

## Wrong data
### DBMS가 막을 수 있는 Wrong data
- DBMS는 기본적으로 **무결성 제약 조건(Integrity constraints)** 을 통해 잘못된 데이터를 미연에 방지할 수 있다. 그럼에도 불구하고 관리자가 제약 조건을 명확히 명시하지 않으면 잘못된 데이터가 발생할 수 있다.
- **ACID(Atomicity, Consistency, Isolation, Durability)** 트랜젝션 관리 기능을 DBMS가 일부 혹은 모두 지원하지 않는 경우도 존재한다. (특히 NoSQL)
### DBMS가 막을 수 없는 Wrong data
- 잘못된 **non-primitive data**
	- `int`, `float`, `boolean` 과 같은 **primitive data**가 아닌 다른 여러 종류의 데이터가 될 수 있으며, 이는 무결성 제약 조건으로 막을 수 없다.
- **하나의 Entity** 내에서의 논리적 Wrong data
	- **오탈자** 혹은 **다른 속성의 값과 서로 바뀐 경우** 논리적 Wrong data가 발생할 수 있다.
- **여러 Entity** 간 일관성 문제

## Unusable data
- **모호한 의미(ambiguous)** 를 가진 데이터
	- 데이터의 **맥락**이 부족함 (ex. 경기도 광주/ 전라도 광주, 경기도인지 전라도인지에 대한 맥락)
- **여러 유효한 표현 방식**을 가진 데이터
	- **사람 이름**이나 **회사 이름**은 다양한 방식으로 표현할 수 있다. (ex. Microsoft/ MS)
	- 여기서 포인트는 두 표현 방식 모두 **맞는 표현**이라는 것!
- **중복(redundant)** 되어 사용할 수 없는 데이터
	- 여러 소스에서 데이터를 수집하다 보면 **중복**이 발생할 수 있다.
	- **상관 관계(Correlation)** 를 분석하면 중복되는 데이터를 검출하는 데 도움이 될 수 있다.
- **표준(standard)** 을 따르지 않아 사용할 수 없는 데이터
	- **서로 다른** 데이터 타입을 사용하는 경우
		- 어떤 테이블은 신용카드 번호를 하나의 **정수**로 표현하고, 어떤 테이블은 dash로 이어진 **문자열**로 표현한다.
	- **불완전한 복합 데이터(Incomplete compound data)**
		- 전화번호와 같이 dash로 연결되어 있는 데이터의 경우, 혹은 주소와 같이 계층적인 데이터의 경우에서 **형식이 완전하지 않은 경우**가 있을 수 있음. 형식이 제각기 다르면 사용할 수 없는 데이터가 된다.
- 데이터베이스 통합(Integration) 과정에서 발생하는 **Mismatch**
	- 서로 다른 데이터를 통합하는 경우, 각 테이블이 데이터를 표현하는 방식이 다르거나, 문화적 배경이 달라서 발생하는 차이가 있을 수 있다. 그 예시는 아래에서 확인할 수 있다.
		- Data types : 같은 feature이지만 서로 다른 데이터 타입을 사용할 수 있다.
		- Representation formats : 숫자를 표현하는 방식이 10진수/16진수 등 다양하다.
		- Date and time formats : 나라마다 날짜 및 시간을 표현하는 형식이 다르다.
		- Measurement units : 나라마다 주로 사용하는 단위가 다르다.
		- Monetary units : 나라마다 주로 사용하는 화폐 단위도 다르다.

## Outliers
- **anomaly, noisy data**라고도 불리는 이상치 데이터는 "비슷한" 데이터 그룹에 속하지 않는 데이터이다. 다시 말해 다른 관측값들과는 **거리가 먼** 관측값이다.
- 이상치가 통계적으로 중요하지 않다면 **제거**하는 것이 가장 간단한 방법이지만, 오히려 이상치가 분석에서 **중요한 역할**을 하는 경우도 있다.
	- Outlier로 인해 **에러가 발생**하는 경우
		- 데이터의 분포가 정규분포를 따라야 하지만 이상치로 인해 **치우치는(skewed)** 경우를 예로 들 수 있다.
	- Outlier가 중요한 상황이 **아닌** 경우
		- 해당 데이터의 **가중치**를 조정하여 다른 데이터보다 적게 둔다.
	- Outlier가 **중요한** 상황인 경우
		- **결함 검출(fraud detection)** 과 같이 무언가 특이점을 찾아야 하는 경우에는 이상치가 **"찾아야 하는 대상"** 이 되므로 중요한 역할을 한다.
- 이상치를 다루는 것은 Data Value Change 단계 뿐 아니라 **Data Reduction** 단계에서도 중요하게 다뤄진다.

# Text preprocessing
세상에 존재하는 대부분의 지식은 텍스트로 이루어져 있기 때문에, 텍스트는 매우 중요한 데이터이다. 그러나 data science 및 ML분야에서는 **수치적 데이터(numerical data)** 에 중점을 두고 있다. 따라서 텍스트 데이터 역시 **수치화** 할 필요성이 있다.
## Noise removal
- 웹에서 **말뭉치(corpus)** 를 수집할 때 불필요한 header/ footer를 제거할 필요가 있다.
- 추가로 불필요한 **markup** 형식이나 **metadata**가 있다면 제거해야 한다.
## Tokenization/ Segmentation/ Lexical analysis
- 텍스트는 길이가 길기 때문에 작은 조각으로 쪼개는 작업(Tokenization)이 필요하다.
- **구두점(punctuation mark)** 등으로 인해 쪼개는 작업이 어려운 경우도 있다.
	- "Dr. Ford" 에서 구두점 기호가 있지만 문장이 끝난 것은 아니다.
	- "isn't"/ "she's"/ "Hawai'i" 에서 각 어퍼스트로피는 다른 역할이다.
## Normalization
- 텍스트 조각(token)들을 같은 선상에서 바라봄으로써 텍스트 유사도/ 단어 빈도수 측정 등을 **합리적**으로 계산하기 위해 정규화는 꼭 필요한 작업이다. 아래에서 **주요한 세 가지 작업**을 알아보자.
	- **어간 추출(word stemming)**
		- "갔어", "갔니" -> "가다"
	- **표제어 추출(word lemmatization)**
		- better -> good/ ran -> run/ finest -> fine
	- 기타 목적에 따른 모든 것...
		- upper, lower case 통일
		- 구두점 제거
		- 숫자 표현 통일 (7 -> seven)
		- **불용어(stop words)** 제거 ("a", "the", "is", "a", ...)

# Data discretization
Data Binning, 즉 **이산화**라고도 불린다. 데이터를 정렬하고 구간을 나누어 수치형 데이터(numerical data)를 **범주형 데이터(categorical data)로 변경**한다. 보통 한 구간에 동일한 빈도수를 구성하도록 partition을 나누는데, 이를 **equal-frequency binning**이라고 한다.
- 예시: 나이 데이터를 범주화하여 20대, 30대, 40대, ... 로 나눌 수 있다.
- 각 구간에서의 값들을 다음의 값들로 **평활화(smoothing)** 할 수 있다.
	- bin means(구간 평균)
	- bin median(구간 중앙값)
	- bin boundaries(구간 최대/최소값)
- 그렇다면 **이산화의 장점**은 무엇일까?
	- 데이터를 단순화하여 더 **직관적으로 해석**할 수 있음
	- 이상치의 영향이 줄어들어 **노이즈 완화** 효과를 얻을 수 있음
	- binning을 통해 차원을 줄여 모델을 **단순화**할 수 있음

# Data normalization/standardization
서로 다른 범위를 가진 데이터를 같은 범위로 맞추는 작업이다. 혹은 대략적인 정규 분포를 따르게 하도록 하기 위해 수행하기도 한다.
\*참고: *data scaling = data normalization = data standardization*

## 왜 데이터의 범위를 조정해야 하는가?
- **머신러닝 알고리즘**은 절대적으로 **큰 값에 더 많은 가중치**를 둘 가능성이 있기 때문이다.
- 키와 몸무게를 예로 들면, 키는 몸무게에 비해 절대적으로 큰 값을 가지고 있다. 하지만 키가 몸무게보다 항상 중요한 것은 아니다. 두 데이터를 **동일 선상에서 판단**해야 올바른 분석이라고 할 수 있을 것이다.
- 머신러닝 알고리즘은 데이터가 비슷한 크기 범위를 가지거나, 분포가 정규 분포와 가까울수록 **성능** 및 **수렴 속도** 측면에서 긍정적이다.
## Normalization/Standardization 방법에는 어떤 것들이 있는가?
- **min-max scaling**
	- 일반적으로 가장 많이 사용되는 정규화 방법론으로, 모든 데이터의 값을 **0과 1사이**로 조정한다.
- **z-score (standard) scaling**
	- feature들을 **평균 0, 표준편차 1**을 가지는 가우시안 분포(Gaussian distribution)의 특성을 갖도록 조정한다. 데이터들은 **공통된 스케일**을 가지게 되며, 해당 데이터의 표준 편차가 얼마나 큰지, 즉 **평균에서 얼마나 떨어져 있는지**를 반영한다.
- **robust scaling**
	- min-max scaling과 비슷하지만, 사용하는 목적이 다르다. robust scaling은 데이터를 **사분위수 범위(IQR, Inter-Quartile Range)** 로 조정하기 위해 사용한다.
	- 이 방법은 **이상치(outlier)에 강하다.** min-max scaling이나 z-score scaling의 경우 극단적으로 크거나 작은 값이 많은 영향을 미치지만, robust scaling은 **IQR**을 기준으로 하기 때문에 **이상치와는 관계없이** 데이터가 고르게 분포될 수 있다.
	- 따라서 불필요한 이상치가 많은 경우에 효과적으로 활용될 수 있다.
- **vector normalizer (direction cosine/ coordinate normalizer)**
	- 벡터의 크기를 1로 만드는 정규화 기법이다. 원래의 방향은 **유지**하면서 벡터의 **크기만 표준화**하는 것이다. 머신 러닝에서 **벡터의 방향**에만 관심이 있고 크기는 중요하지 않은 경우가 있기 때문이다.
	- **방향 코사인(Direction cosine)** 으로 부르기도 한다. 벡터의 크기를 정규화 하려면 해당 벡터를 자기 자신의 크기로 나누어야 하는데, 이 과정이 방향 코사인을 구하는 과정과 동일하기 때문이다.
- **decimal scaling**
	- 범위 내 최대 절대값의 자릿수를 j 라고 할 때, 전체 숫자를 10의 j승으로 나누어 정규화한다.
	- 예를 들어, -986~917 범위 내 숫자가 있다고 가정하면, 최대값인 917의 자릿수가 3이므로 `j=3`이다. 따라서 모든 숫자를 10의 3승인 1000으로 나누게 되므로, 데이터는 **-0.986~0.917**로 정규화된다.
- **log normalization**
	- 데이터에 **로그**를 취하여 정규화하는 방식으로, 양수 범위에서 정의되는 데이터가 한 쪽으로 **치우쳐(skewed)** 있거나, 데이터가 지수함수적으로 증가하는 경우 이를 **선형화**할 때 사용하면 효과적이다.
	- 이상치에도 일정 부분 효과가 있지만, **극단적인 이상치**가 있는 경우 혹은 **이상치의 개수가 많은** 경우에는 한계가 있으므로 Robust scaling을 사용하는 것이 더 효과적일 수 있다.
## 상황에 따라 효과적일 수 있는 정규화 방법
- **High entropy (Highly random data)**
	- 데이터가 너무 랜덤하게 분포되어 있다면 **\[-1, 1\]** 사이로 정규화하는 것이 효과적일 수 있다.
- **Low entropy (Less random data)**
	- 데이터가 조밀하게 분포되어 있을 경우에는 **z-score scaling**이 효과적일 수 있다.
	- 정규 분포와 가깝게 분포되어 있을 경우에도 z-score scaling을 사용하면 좋다.

# Encoding for data mining algorithms
**비수치적(non-numeric)** 데이터를 머신러닝 알고리즘에 적용시키기 위해서는, 이를 배열에 **대응(Map)** 시키는 전처리 과정이 필요하다. 비수치적 데이터의 예시는 다음과 같다.
- 범주형 데이터(categorical data)
- 텍스트 데이터(text data)
- 이미지 데이터(image data)

또한, 데이터를 신경망에 입력하기 위해서 이전에 소개한 여러 정규화 방법을 사용하여 **\[0, 1\] 사이의 값으로 변환**하는 과정도 필요하다. 


>(5) Reference

